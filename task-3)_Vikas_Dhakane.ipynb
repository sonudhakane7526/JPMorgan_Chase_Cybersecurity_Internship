{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1df29eda",
   "metadata": {},
   "source": [
    "Step 0. Go to the Ham or Spam dataset [website](http://www2.aueb.gr/users/ion/data/enron-spam/index.html) and download the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f392b2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget http://www.aueb.gr/users/ion/data/enron-spam/preprocessed/enron1.tar.gz\n",
    "# !tar -xvzf enron1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf32cfce",
   "metadata": {},
   "source": [
    "Step 1. Traverse the dataset and create a Pandas dataframe. This is already done for you. You should recognize Pandas from task 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20c5d195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped 2649.2004-10-27.GP.spam.txt\n",
      "skipped 0754.2004-04-01.GP.spam.txt\n",
      "skipped 2042.2004-08-30.GP.spam.txt\n",
      "skipped 3304.2004-12-26.GP.spam.txt\n",
      "skipped 4142.2005-03-31.GP.spam.txt\n",
      "skipped 3364.2005-01-01.GP.spam.txt\n",
      "skipped 4201.2005-04-05.GP.spam.txt\n",
      "skipped 2140.2004-09-13.GP.spam.txt\n",
      "skipped 2248.2004-09-23.GP.spam.txt\n",
      "skipped 4350.2005-04-23.GP.spam.txt\n",
      "skipped 4566.2005-05-24.GP.spam.txt\n",
      "skipped 2526.2004-10-17.GP.spam.txt\n",
      "skipped 1414.2004-06-24.GP.spam.txt\n",
      "skipped 2698.2004-10-31.GP.spam.txt\n",
      "skipped 5105.2005-08-31.GP.spam.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def read_spam():\n",
    "    category = 'spam'\n",
    "    directory = './enron1/spam'\n",
    "    return read_category(category, directory)\n",
    "\n",
    "def read_ham():\n",
    "    category = 'ham'\n",
    "    directory = './enron1/ham'\n",
    "    return read_category(category, directory)\n",
    "\n",
    "def read_category(category, directory):\n",
    "    emails = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        with open(os.path.join(directory, filename), 'r') as fp:\n",
    "            try:\n",
    "                content = fp.read()\n",
    "                emails.append({'name': filename, 'content': content, 'category': category})\n",
    "            except:\n",
    "                print(f'skipped {filename}')\n",
    "    return emails\n",
    "\n",
    "ham = read_ham()\n",
    "spam = read_spam()\n",
    "\n",
    "df = pd.DataFrame.from_records(ham)\n",
    "df = df.append(pd.DataFrame.from_records(spam))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1c23fd",
   "metadata": {},
   "source": [
    "Step 2. Data cleaning is a critical part of machine learning. You and I can recognize that 'Hello' and 'hello' are the same word but a machine does not know this a priori. Therefore, we can 'help' the machine by conducting such normalization steps for it. Write a function `preprocessor` that takes in a string and replaces all non alphabet characters with a space and then lowercases the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c447c901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocessor(e):\n",
    "    return re.sub('[^A-Za-z]', ' ', e).lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba32521d",
   "metadata": {},
   "source": [
    "Step 3. We will now train the machine learning model. All the functions that you will need are imported for you. The instructions explain how the work and hint at which functions to use. You will likely need to refer to the scikit learn documentation to see how exactly to invoke the functions. It will be handy to keep that tab open."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1442d377",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raja/opt/miniconda3/envs/forage/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "0.9767441860465116\n",
      "\n",
      "Confusion Matrix:\n",
      "[[726  16]\n",
      " [  8 282]]\n",
      "\n",
      "Detailed Statistics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      0.98      0.98       742\n",
      "        spam       0.95      0.97      0.96       290\n",
      "\n",
      "    accuracy                           0.98      1032\n",
      "   macro avg       0.97      0.98      0.97      1032\n",
      "weighted avg       0.98      0.98      0.98      1032\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "vectorizer = CountVectorizer(preprocessor=preprocessor)\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(df[\"content\"],df[\"category\"],test_size=0.2,random_state=1)\n",
    "\n",
    "X_train_df = vectorizer.fit_transform(X_train)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_df,y_train)\n",
    "\n",
    "\n",
    "X_test_df = vectorizer.transform(X_test)\n",
    "y_pred = model.predict(X_test_df)\n",
    "\n",
    "\n",
    "print(f'Accuracy:\\n{accuracy_score(y_test,y_pred)}\\n')\n",
    "print(f'Confusion Matrix:\\n{confusion_matrix(y_test,y_pred)}\\n')\n",
    "print(f'Detailed Statistics:\\n{classification_report(y_test,y_pred)}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9674d032",
   "metadata": {},
   "source": [
    "Step 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b7d78c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.061288626482321 http\n",
      "0.8171514687730761 prices\n",
      "0.7939415840701827 no\n",
      "0.7566963969909719 remove\n",
      "0.7217203117456084 removed\n",
      "0.6916604371247976 off\n",
      "0.6795655311184997 hello\n",
      "0.6537526327583835 mobile\n",
      "0.6338517298349112 more\n",
      "0.6164935024552568 message\n",
      "\n",
      "-1.5437128445038255 attached\n",
      "-1.5401915054858066 daren\n",
      "-1.5015980523929702 enron\n",
      "-1.3008715559663415 thanks\n",
      "-1.2496145154883649 doc\n",
      "-1.1804716676252003 meter\n",
      "-1.0830241823597755 xls\n",
      "-1.0658050172716116 hpl\n",
      "-1.04603390757529 neon\n",
      "-1.0367514536221871 deal\n"
     ]
    }
   ],
   "source": [
    "features = vectorizer.get_feature_names_out()\n",
    "\n",
    "importance = model.coef_[0]\n",
    "\n",
    "l = list(enumerate(importance))\n",
    "print()\n",
    "l.sort(key=lambda e: e[1], reverse=True)\n",
    "for i,imp in l[:10]:\n",
    "    print(imp, features[i])\n",
    "print()\n",
    "l.sort(key=lambda e: -e[1], reverse=True)\n",
    "for i,imp in l[:10]:\n",
    "    print(imp, features[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d267e7ad",
   "metadata": {},
   "source": [
    "All Done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
